{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26552761bb0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26552761bb0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField,LabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy , Average\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "from allennlp.data.iterators import BucketIterator, BasicIterator\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder,PytorchSeq2VecWrapper\n",
    "from torch.nn import LogSoftmax\n",
    "from torch.nn.modules import NLLLoss\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "    StanfordSentimentTreeBankDatasetReader\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
    "from allennlp.training.trainer import Trainer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for PoS tagging data, one word per line and its label \n",
    "    Doveyski , Russian.txt\n",
    "    \"\"\"\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "    def text_to_instance(self, tokens: List[Token], label: str =None) -> Instance:\n",
    "        word_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"word\": word_field}\n",
    " \n",
    "        if label is None:\n",
    "            return Instance(fields)\n",
    "        \n",
    "        \n",
    "        label_field = LabelField(label=label)\n",
    "        fields[\"label\"] = label_field\n",
    "            \n",
    "        return Instance(fields)   \n",
    "    \n",
    "    def findFiles(self,path): \n",
    "        return glob.glob(path)\n",
    "    \n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        \n",
    "        with open(file_path,encoding='utf-8') as f:\n",
    "            data=json.loads(f.read())\n",
    "            for item in data:\n",
    "                yield self.text_to_instance([Token(w) for w in item['headline'].split(\" \") + item['short_description'].split(\" \")], item['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00,  3.09it/s]\n",
      "20000it [00:03, 5784.11it/s]\n",
      "3000it [00:00, 29783.10it/s]\n"
     ]
    }
   ],
   "source": [
    "elmo_token_indexer = ELMoTokenCharactersIndexer()\n",
    "reader =NewsDatasetReader(\n",
    "        token_indexers={'tokens': elmo_token_indexer})\n",
    "train_dataset = reader.read('train_.txt')\n",
    "validation_dataset=reader.read('validation_.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "    StanfordSentimentTreeBankDatasetReader\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "# from realworldnlp.predictors import SentenceClassifierPredictor\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "# Model in AllenNLP represents a model that is trained.\n",
    "#@Model.register(\"lstm_classifier\")\n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary,\n",
    "                 positive_label: int = 4) -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                      out_features=128)\n",
    "\n",
    "        # Monitor the metrics - we use accuracy, as well as prec, rec, f1 for 4 (very positive)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.f1_measure = F1Measure(positive_label)\n",
    "\n",
    "        # We use the cross entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                word: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(word)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(word)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.linear(encoder_out)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            self.f1_measure(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        precision, recall, f1_measure = self.f1_measure.get_metric(reset)\n",
    "        return {'accuracy': self.accuracy.get_metric(reset),\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_measure': f1_measure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json'\n",
    "weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 23000/23000 [00:00<00:00, 173396.54it/s]\n"
     ]
    }
   ],
   "source": [
    "elmo_embedder = ElmoTokenEmbedder(options_file, weight_file)\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)\n",
    "\n",
    "# Pass in the ElmoTokenEmbedder instance instead\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": elmo_embedder})\n",
    "\n",
    "# The dimension of the ELMo embedding will be 2 x [size of LSTM hidden states]\n",
    "elmo_embedding_dim = 256\n",
    "lstm = PytorchSeq2VecWrapper(\n",
    "    torch.nn.LSTM(elmo_embedding_dim, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model = LstmClassifier(word_embeddings, lstm, vocab)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "iterator = BucketIterator(batch_size=100, sorting_keys=[(\"word\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5942, precision: 0.7560, recall: 0.3870, f1_measure: 0.5119, loss: 1.1903 ||: 100%|█| 200/200 [16:42<00:00,  3.32s/it]\n",
      "accuracy: 0.7273, precision: 0.8696, recall: 0.7092, f1_measure: 0.7812, loss: 0.7609 ||: 100%|█| 30/30 [02:12<00:00,  8.70s/it]\n",
      "accuracy: 0.7402, precision: 0.7975, recall: 0.7756, f1_measure: 0.7864, loss: 0.7093 ||: 100%|█| 200/200 [17:09<00:00,  6.82s/it]\n",
      "accuracy: 0.7643, precision: 0.7940, recall: 0.8475, f1_measure: 0.8199, loss: 0.6416 ||: 100%|█| 30/30 [02:16<00:00,  8.93s/it]\n",
      "accuracy: 0.7677, precision: 0.8425, recall: 0.8149, f1_measure: 0.8285, loss: 0.6346 ||: 100%|█| 200/200 [1:04:52<00:00, 51.86s/it]\n",
      "accuracy: 0.7740, precision: 0.7830, recall: 0.8830, f1_measure: 0.8300, loss: 0.6166 ||: 100%|█| 30/30 [24:18<00:00, 116.50s/it]\n",
      "accuracy: 0.7844, precision: 0.8448, recall: 0.8294, f1_measure: 0.8370, loss: 0.5856 ||: 100%|█| 200/200 [2:42:27<00:00, 48.76s/it]   \n",
      "accuracy: 0.7783, precision: 0.7405, recall: 0.9007, f1_measure: 0.8128, loss: 0.5911 ||: 100%|█| 30/30 [23:07<00:00, 93.75s/it]\n",
      "accuracy: 0.7975, precision: 0.8645, recall: 0.8450, f1_measure: 0.8547, loss: 0.5562 ||: 100%|█| 200/200 [4:14:21<00:00, 102.58s/it]  \n",
      "accuracy: 0.7990, precision: 0.8454, recall: 0.8723, f1_measure: 0.8586, loss: 0.5519 ||: 100%|█| 30/30 [38:04<00:00, 119.79s/it]\n",
      "accuracy: 0.8063, precision: 0.8722, recall: 0.8520, f1_measure: 0.8620, loss: 0.5321 ||: 100%|█| 200/200 [3:39:33<00:00, 44.50s/it]  \n",
      "accuracy: 0.7977, precision: 0.8443, recall: 0.8652, f1_measure: 0.8546, loss: 0.5438 ||: 100%|█| 30/30 [31:03<00:00, 115.85s/it]\n",
      "accuracy: 0.8158, precision: 0.8697, recall: 0.8654, f1_measure: 0.8675, loss: 0.5093 ||: 100%|█| 200/200 [27:24<00:00,  4.42s/it] \n",
      "accuracy: 0.8027, precision: 0.8115, recall: 0.9007, f1_measure: 0.8538, loss: 0.5450 ||: 100%|█| 30/30 [02:10<00:00,  8.61s/it]\n",
      "accuracy: 0.8209, precision: 0.8850, recall: 0.8741, f1_measure: 0.8795, loss: 0.4893 ||: 100%|█| 200/200 [16:21<00:00,  3.53s/it]\n",
      "accuracy: 0.8030, precision: 0.8379, recall: 0.8617, f1_measure: 0.8497, loss: 0.5354 ||: 100%|█| 30/30 [02:10<00:00,  8.59s/it]\n",
      "accuracy: 0.8295, precision: 0.8852, recall: 0.8794, f1_measure: 0.8823, loss: 0.4657 ||: 100%|█| 200/200 [16:24<00:00,  4.43s/it]\n",
      "accuracy: 0.8063, precision: 0.8407, recall: 0.8794, f1_measure: 0.8596, loss: 0.5246 ||: 100%|█| 30/30 [02:10<00:00,  8.62s/it]\n",
      "accuracy: 0.8354, precision: 0.8955, recall: 0.8811, f1_measure: 0.8882, loss: 0.4517 ||: 100%|█| 200/200 [16:22<00:00,  4.71s/it]\n",
      "accuracy: 0.8140, precision: 0.8311, recall: 0.8901, f1_measure: 0.8596, loss: 0.5239 ||: 100%|█| 30/30 [02:08<00:00,  8.60s/it]\n",
      "accuracy: 0.8407, precision: 0.8996, recall: 0.8918, f1_measure: 0.8957, loss: 0.4346 ||: 100%|█| 200/200 [16:20<00:00,  5.88s/it]\n",
      "accuracy: 0.8040, precision: 0.8561, recall: 0.8652, f1_measure: 0.8607, loss: 0.5351 ||: 100%|█| 30/30 [02:08<00:00,  8.44s/it]\n",
      "accuracy: 0.8455, precision: 0.9049, recall: 0.9010, f1_measure: 0.9029, loss: 0.4226 ||: 100%|█| 200/200 [16:20<00:00,  4.45s/it]\n",
      "accuracy: 0.8147, precision: 0.8754, recall: 0.8723, f1_measure: 0.8739, loss: 0.5206 ||: 100%|█| 30/30 [02:09<00:00,  8.62s/it]\n",
      "accuracy: 0.8529, precision: 0.9032, recall: 0.8994, f1_measure: 0.9013, loss: 0.4039 ||: 100%|█| 200/200 [16:24<00:00,  8.48s/it]\n",
      "accuracy: 0.8133, precision: 0.8419, recall: 0.8688, f1_measure: 0.8551, loss: 0.5161 ||: 100%|█| 30/30 [02:07<00:00,  8.76s/it]\n",
      "accuracy: 0.8592, precision: 0.9174, recall: 0.9031, f1_measure: 0.9102, loss: 0.3856 ||: 100%|█| 200/200 [16:24<00:00,  4.65s/it]\n",
      "accuracy: 0.8150, precision: 0.8448, recall: 0.8688, f1_measure: 0.8566, loss: 0.5278 ||: 100%|█| 30/30 [02:09<00:00,  8.70s/it]\n",
      "accuracy: 0.8623, precision: 0.9093, recall: 0.9117, f1_measure: 0.9105, loss: 0.3770 ||: 100%|█| 200/200 [16:21<00:00,  6.88s/it]\n",
      "accuracy: 0.8183, precision: 0.8989, recall: 0.8511, f1_measure: 0.8743, loss: 0.5275 ||: 100%|█| 30/30 [02:09<00:00,  8.79s/it]\n",
      "accuracy: 0.8663, precision: 0.9215, recall: 0.9166, f1_measure: 0.9191, loss: 0.3618 ||: 100%|█| 200/200 [16:26<00:00,  4.14s/it]\n",
      "accuracy: 0.8187, precision: 0.8934, recall: 0.8617, f1_measure: 0.8773, loss: 0.5276 ||: 100%|█| 30/30 [02:07<00:00,  8.53s/it]\n",
      "accuracy: 0.8760, precision: 0.9251, recall: 0.9236, f1_measure: 0.9243, loss: 0.3407 ||: 100%|█| 200/200 [16:25<00:00,  4.37s/it]\n",
      "accuracy: 0.8063, precision: 0.8985, recall: 0.8475, f1_measure: 0.8723, loss: 0.5644 ||: 100%|█| 30/30 [02:08<00:00,  8.75s/it]\n",
      "accuracy: 0.8807, precision: 0.9239, recall: 0.9209, f1_measure: 0.9224, loss: 0.3302 ||: 100%|█| 200/200 [16:26<00:00,  4.23s/it]\n",
      "accuracy: 0.8203, precision: 0.8939, recall: 0.8369, f1_measure: 0.8645, loss: 0.5264 ||: 100%|█| 30/30 [02:08<00:00,  8.39s/it]\n",
      "accuracy: 0.8829, precision: 0.9315, recall: 0.9220, f1_measure: 0.9267, loss: 0.3213 ||: 100%|█| 200/200 [16:26<00:00,  4.52s/it]\n",
      "accuracy: 0.8087, precision: 0.8966, recall: 0.8298, f1_measure: 0.8619, loss: 0.5882 ||: 100%|█| 30/30 [02:10<00:00,  8.89s/it]\n",
      "accuracy: 0.8887, precision: 0.9374, recall: 0.9349, f1_measure: 0.9361, loss: 0.3031 ||: 100%|█| 200/200 [32:26<00:00,  4.85s/it]\n",
      "accuracy: 0.8203, precision: 0.8551, recall: 0.8582, f1_measure: 0.8566, loss: 0.5403 ||: 100%|█| 30/30 [02:06<00:00,  8.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 12,\n",
       " 'peak_cpu_memory_MB': 0,\n",
       " 'training_duration': '19:02:53',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 19,\n",
       " 'epoch': 19,\n",
       " 'training_accuracy': 0.88865,\n",
       " 'training_precision': 0.9373988127361036,\n",
       " 'training_recall': 0.9348762109795479,\n",
       " 'training_f1_measure': 0.9361358124494247,\n",
       " 'training_loss': 0.30314835950732233,\n",
       " 'training_cpu_memory_MB': 0.0,\n",
       " 'validation_accuracy': 0.8203333333333334,\n",
       " 'validation_precision': 0.8551236749116607,\n",
       " 'validation_recall': 0.8581560283687943,\n",
       " 'validation_f1_measure': 0.8566371681415429,\n",
       " 'validation_loss': 0.5402635181943576,\n",
       " 'best_validation_accuracy': 0.8133333333333334,\n",
       " 'best_validation_precision': 0.8419243986254296,\n",
       " 'best_validation_recall': 0.8687943262411347,\n",
       " 'best_validation_f1_measure': 0.8551483420592868,\n",
       " 'best_validation_loss': 0.5160865028699239}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = predictor.predict(\"Why Disturbing Leaked Video Of Texas Cops May Be Relevant To Jordan Edwards' Killing\")['logits']\n",
    "tag_ids = np.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POLITICS']\n"
     ]
    }
   ],
   "source": [
    "print([model.vocab.get_token_from_index(tag_ids, 'labels')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = ('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json')\n",
    "weight_file = ('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 23000/23000 [00:00<00:00, 147827.30it/s]\n"
     ]
    }
   ],
   "source": [
    "elmo_embedder = ElmoTokenEmbedder(options_file, weight_file)\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)\n",
    "\n",
    "# Pass in the ElmoTokenEmbedder instance instead\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": elmo_embedder})\n",
    "\n",
    "# The dimension of the ELMo embedding will be 2 x [size of LSTM hidden states]\n",
    "elmo_embedding_dim = 1024\n",
    "lstm = PytorchSeq2VecWrapper(\n",
    "    torch.nn.LSTM(elmo_embedding_dim, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model = LstmClassifier(word_embeddings, lstm, vocab)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "iterator = BucketIterator(batch_size=100, sorting_keys=[(\"word\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5189, precision: 0.3298, recall: 0.1097, f1_measure: 0.1646, loss: 1.8364 ||: 100%|█| 200/200 [34:00<00:00, 10.00s/it]\n",
      "accuracy: 0.5917, precision: 0.4444, recall: 0.0519, f1_measure: 0.0930, loss: 1.4797 ||: 100%|█| 30/30 [04:08<00:00, 11.50s/it]\n",
      "accuracy: 0.6219, precision: 0.5157, recall: 0.3827, f1_measure: 0.4394, loss: 1.2943 ||: 100%|█| 200/200 [32:55<00:00, 11.13s/it]\n",
      "accuracy: 0.6160, precision: 0.4026, recall: 0.4026, f1_measure: 0.4026, loss: 1.3369 ||: 100%|█| 30/30 [03:47<00:00, 10.76s/it]\n",
      "accuracy: 0.6625, precision: 0.5787, recall: 0.4889, f1_measure: 0.5300, loss: 1.1269 ||: 100%|█| 200/200 [31:37<00:00,  8.07s/it]\n",
      "accuracy: 0.6247, precision: 0.3832, recall: 0.5325, f1_measure: 0.4457, loss: 1.2834 ||: 100%|█| 30/30 [03:38<00:00, 10.08s/it]\n",
      "accuracy: 0.6895, precision: 0.6603, recall: 0.5648, f1_measure: 0.6088, loss: 1.0160 ||: 100%|█| 200/200 [31:10<00:00, 11.09s/it]\n",
      "accuracy: 0.6373, precision: 0.2812, recall: 0.5844, f1_measure: 0.3797, loss: 1.2506 ||: 100%|█| 30/30 [03:38<00:00, 10.17s/it]\n",
      "accuracy: 0.7196, precision: 0.6967, recall: 0.6324, f1_measure: 0.6630, loss: 0.9088 ||: 100%|█| 200/200 [31:14<00:00,  8.24s/it]\n",
      "accuracy: 0.6530, precision: 0.5161, recall: 0.4156, f1_measure: 0.4604, loss: 1.2359 ||: 100%|█| 30/30 [03:40<00:00, 10.14s/it]\n",
      "accuracy: 0.7441, precision: 0.7269, recall: 0.6429, f1_measure: 0.6824, loss: 0.8147 ||: 100%|█| 200/200 [31:17<00:00,  8.89s/it]\n",
      "accuracy: 0.6480, precision: 0.4500, recall: 0.4675, f1_measure: 0.4586, loss: 1.2427 ||: 100%|█| 30/30 [03:40<00:00, 10.26s/it]\n",
      "accuracy: 0.7703, precision: 0.7453, recall: 0.6896, f1_measure: 0.7164, loss: 0.7272 ||: 100%|█| 200/200 [31:25<00:00,  7.72s/it]\n",
      "accuracy: 0.6470, precision: 0.4432, recall: 0.5065, f1_measure: 0.4727, loss: 1.2477 ||: 100%|█| 30/30 [03:39<00:00, 10.17s/it]\n",
      "accuracy: 0.7921, precision: 0.7722, recall: 0.7001, f1_measure: 0.7344, loss: 0.6530 ||: 100%|█| 200/200 [31:45<00:00,  9.06s/it]\n",
      "accuracy: 0.6567, precision: 0.4605, recall: 0.4545, f1_measure: 0.4575, loss: 1.2795 ||: 100%|█| 30/30 [03:39<00:00, 10.38s/it]\n",
      "accuracy: 0.8139, precision: 0.7972, recall: 0.7386, f1_measure: 0.7668, loss: 0.5792 ||: 100%|█| 200/200 [31:03<00:00,  8.62s/it]\n",
      "accuracy: 0.6460, precision: 0.4861, recall: 0.4545, f1_measure: 0.4698, loss: 1.3380 ||: 100%|█| 30/30 [03:29<00:00,  9.70s/it]\n",
      "accuracy: 0.8346, precision: 0.8127, recall: 0.7748, f1_measure: 0.7933, loss: 0.5101 ||: 100%|█| 200/200 [29:58<00:00,  7.92s/it]\n",
      "accuracy: 0.6523, precision: 0.5224, recall: 0.4545, f1_measure: 0.4861, loss: 1.3378 ||: 100%|█| 30/30 [03:29<00:00,  9.84s/it]\n",
      "accuracy: 0.8531, precision: 0.8067, recall: 0.7643, f1_measure: 0.7849, loss: 0.4543 ||: 100%|█| 200/200 [30:53<00:00,  9.14s/it]\n",
      "accuracy: 0.6543, precision: 0.5789, recall: 0.4286, f1_measure: 0.4925, loss: 1.3840 ||: 100%|█| 30/30 [03:35<00:00,  9.99s/it]\n",
      "accuracy: 0.8732, precision: 0.8414, recall: 0.8110, f1_measure: 0.8259, loss: 0.3961 ||: 100%|█| 200/200 [31:16<00:00,  8.74s/it]\n",
      "accuracy: 0.6573, precision: 0.5532, recall: 0.3377, f1_measure: 0.4194, loss: 1.4716 ||: 100%|█| 30/30 [03:42<00:00, 10.19s/it]\n",
      "accuracy: 0.8850, precision: 0.8627, recall: 0.8285, f1_measure: 0.8452, loss: 0.3532 ||: 100%|█| 200/200 [31:50<00:00,  8.98s/it]\n",
      "accuracy: 0.6560, precision: 0.4141, recall: 0.5325, f1_measure: 0.4659, loss: 1.4589 ||: 100%|█| 30/30 [03:45<00:00, 10.61s/it]\n",
      "accuracy: 0.9037, precision: 0.8593, recall: 0.8483, f1_measure: 0.8538, loss: 0.3068 ||: 100%|█| 200/200 [32:15<00:00,  9.03s/it]\n",
      "accuracy: 0.6397, precision: 0.4691, recall: 0.4935, f1_measure: 0.4810, loss: 1.5349 ||: 100%|█| 30/30 [03:48<00:00, 10.60s/it]\n",
      "accuracy: 0.9154, precision: 0.8852, recall: 0.8635, f1_measure: 0.8742, loss: 0.2718 ||: 100%|█| 200/200 [32:27<00:00,  9.77s/it]\n",
      "accuracy: 0.6277, precision: 0.5645, recall: 0.4545, f1_measure: 0.5036, loss: 1.5845 ||: 100%|█| 30/30 [03:48<00:00, 10.68s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 4,\n",
       " 'peak_cpu_memory_MB': 0,\n",
       " 'training_duration': '08:14:28',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 13,\n",
       " 'epoch': 13,\n",
       " 'training_accuracy': 0.9037,\n",
       " 'training_precision': 0.859338061465721,\n",
       " 'training_recall': 0.8483080513418904,\n",
       " 'training_f1_measure': 0.8537874339400557,\n",
       " 'training_loss': 0.3067641580849886,\n",
       " 'training_cpu_memory_MB': 0.0,\n",
       " 'validation_accuracy': 0.6396666666666667,\n",
       " 'validation_precision': 0.4691358024691358,\n",
       " 'validation_recall': 0.4935064935064935,\n",
       " 'validation_f1_measure': 0.48101265822779804,\n",
       " 'validation_loss': 1.534892777601878,\n",
       " 'best_validation_accuracy': 0.653,\n",
       " 'best_validation_precision': 0.5161290322580645,\n",
       " 'best_validation_recall': 0.4155844155844156,\n",
       " 'best_validation_f1_measure': 0.46043165467620956,\n",
       " 'best_validation_loss': 1.2359104375044505}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common import JsonDict\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.models import Model\n",
    "from allennlp.predictors import Predictor\n",
    "from overrides import overrides\n",
    "import numpy as np\n",
    "\n",
    "class NewsClassifierPredictor(Predictor):\n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "        self.model = model\n",
    "        self._tokenizer = SpacyWordSplitter(language='en_core_web_sm', pos_tags=True)\n",
    "\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        scores = self.predict_json({\"sentence\" : sentence})['logits']\n",
    "        label_id = np.argmax(scores)\n",
    "        return self.model.vocab.get_token_from_index(label_id, 'labels')\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        sentence = json_dict[\"sentence\"]\n",
    "        tokens = self._tokenizer.split_words(sentence)\n",
    "        return self._dataset_reader.text_to_instance(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = NewsClassifierPredictor(model, dataset_reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WORLD NEWS'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
