{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('News_Category_Dataset_v2.json') as json_file:  \n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [ x for x in data if x['category'] in ['HEALTHY LIVING','WORLD NEWS','COMEDY','SPORTS','BLACK VOICES'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23458"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sample[:20000]\n",
    "validation = sample[20000:23000]\n",
    "test = sample[23000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'HEALTHY LIVING':1,'WORLD NEWS':2,'COMEDY':3,'SPORTS':4,'BLACK VOICES':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L=[]\n",
    "k=[]\n",
    "C=[]\n",
    "for item in train:\n",
    "    c=d[item['category']]\n",
    "    C.append(c)\n",
    "    exp=[item['headline'],item['short_description']]\n",
    "    EXP=item['headline']+item['short_description']\n",
    "    L.append(exp)\n",
    "    k.append(EXP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=[]\n",
    "C_T=[ ]\n",
    "for item in test:\n",
    "    c=d[item['category']]\n",
    "    EXP=item['headline']+item['short_description']\n",
    "    T.append(EXP)\n",
    "    C_T.append(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(k)\n",
    "test_df = pd.DataFrame(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['category']=C\n",
    "test_df['category']=C_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns=[\"sentence\",\"category\"]\n",
    "test_df.columns=[\"sentence\",\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Late Night' Writer Expertly Schools John Kell...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feeling Draped In An Overcast Of 'Diversity'Di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4 Skin Issues That Can Be Solved By A Good Nig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The U.S. Will Probably Have At Least One Zika ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump’s Plan To Kill The Iran Deal? Outsourcin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  category\n",
       "0  'Late Night' Writer Expertly Schools John Kell...         3\n",
       "1  Feeling Draped In An Overcast Of 'Diversity'Di...         0\n",
       "2  4 Skin Issues That Can Be Solved By A Good Nig...         1\n",
       "3  The U.S. Will Probably Have At Least One Zika ...         1\n",
       "4  Trump’s Plan To Kill The Iran Deal? Outsourcin...         2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adil\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import keras.layers as layers\n",
    "\n",
    "from collections import Counter\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Input, Embedding, BatchNormalization, LSTM, Dense, Concatenate,Activation\n",
    "from keras.models import Model\n",
    "\n",
    "# from keras.utils import plot_mode\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce TensorFlow logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Instantiate the elmo model\n",
    "elmo_module = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter of max word length\n",
    "time_steps = 100\n",
    "# building vocabulary from dataset\n",
    "def build_vocabulary(sentence_list):\n",
    "    unique_words = \" \".join(sentence_list).strip().split()\n",
    "    word_count = Counter(unique_words).most_common()\n",
    "    vocabulary = {}\n",
    "    for word, _ in word_count:\n",
    "        vocabulary[word] = len(vocabulary)        \n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "# Get vocabulary vectors from document list\n",
    "# Vocabulary vector, Unknown word is 1 and padding is 0\n",
    "# INPUT: raw sentence list\n",
    "# OUTPUT: vocabulary vectors list\n",
    "def get_voc_vec(document_list, vocabulary):    \n",
    "    voc_ind_sentence_list = []\n",
    "    for document in document_list:\n",
    "        voc_idx_sentence = []\n",
    "        word_list = document.split()\n",
    "        \n",
    "        for w in range(time_steps):\n",
    "            if w < len(word_list):\n",
    "                # pickup vocabulary id and convert unknown word into 1\n",
    "                voc_idx_sentence.append(vocabulary.get(word_list[w], -1) + 2)\n",
    "            else:\n",
    "                # padding with 0\n",
    "                voc_idx_sentence.append(0)\n",
    "            \n",
    "        voc_ind_sentence_list.append(voc_idx_sentence)\n",
    "        \n",
    "    return np.array(voc_ind_sentence_list)\n",
    "\n",
    "\n",
    "vocabulary = build_vocabulary(train_df[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch generator\n",
    "def batch_iter(data, labels, batch_size, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    print(\"batch_size\", batch_size)\n",
    "    print(\"num_batches_per_epoch\", num_batches_per_epoch)\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                                \n",
    "                X_voc = get_voc_vec(shuffled_data[start_index: end_index], vocabulary)\n",
    "                                \n",
    "                sentence_split_list = []\n",
    "                sentence_split_length_list = []\n",
    "            \n",
    "                for sentence in shuffled_data[start_index: end_index]:    \n",
    "                    sentence_split = sentence.split()\n",
    "                    sentence_split_length = len(sentence_split)\n",
    "                    sentence_split += [\"NaN\"] * (time_steps - sentence_split_length)\n",
    "                    \n",
    "                    sentence_split_list.append((\" \").join(sentence_split))\n",
    "                    sentence_split_length_list.append(sentence_split_length)\n",
    "        \n",
    "                X_elmo = np.array(sentence_split_list)\n",
    "\n",
    "                X =  X_elmo\n",
    "                y = shuffled_labels[start_index: end_index]\n",
    "                \n",
    "                yield X, y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed elmo method\n",
    "def make_elmo_embedding(x):\n",
    "    embeddings = elmo_module(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               590336    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 595,077\n",
      "Trainable params: 593,029\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# elmo embedding dimension\n",
    "elmo_dim = 1024\n",
    "\n",
    "# Input Layers\n",
    "#word_input = Input(shape=(None, ), dtype='int32')  # (batch_size, sent_length)\n",
    "elmo_input = Input(shape=(None, ), dtype=tf.string)  # (batch_size, sent_length, elmo_size)\n",
    "\n",
    "# Hidden Layers\n",
    "#word_embedding = Embedding(input_dim=len(vocabulary), output_dim=128, mask_zero=True)(word_input)\n",
    "elmo_embedding = layers.Lambda(make_elmo_embedding, output_shape=(None, elmo_dim))(elmo_input)\n",
    "#word_embedding = Concatenate()([word_embedding, elmo_embedding])\n",
    "word_embedding = BatchNormalization()(elmo_embedding)\n",
    "x = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(word_embedding)\n",
    "\n",
    "# Output Layer\n",
    "\n",
    "predict = Dense(units=5, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=[elmo_input], outputs=predict)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#plot_model(model, to_file=\"model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 100\n",
    "# Create datasets (Only take up to time_steps words for memory)\n",
    "train_text = train_df['sentence'].tolist()\n",
    "train_text = [' '.join(t.split()[0:time_steps]) for t in train_text]\n",
    "train_text = np.array(train_text)\n",
    "train_label = np.array(train_df['category'].tolist())\n",
    "\n",
    "test_text = test_df['sentence'].tolist()\n",
    "test_text = [' '.join(t.split()[0:time_steps]) for t in test_text]\n",
    "test_text = np.array(test_text)\n",
    "test_label = np.array(test_df['category'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 32\n",
      "num_batches_per_epoch 625\n",
      "batch_size 32\n",
      "num_batches_per_epoch 15\n"
     ]
    }
   ],
   "source": [
    "# mini-batch size\n",
    "batch_size = 32\n",
    "\n",
    "train_steps, train_batches = batch_iter(train_text,\n",
    "                                        np.array(train_df[\"category\"]),\n",
    "                                        batch_size)\n",
    "valid_steps, valid_batches = batch_iter(test_text,\n",
    "                                        np.array(test_df[\"category\"]),\n",
    "                                        batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "   64/18000 [..............................] - ETA: 2:42:09 - loss: 1.5945 - acc: 0.2500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-11a95ebc0155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcheckpointer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m history = model.fit([train_text], batch_size=64, y=to_categorical(train_label), verbose=1, validation_split=0.10, \n\u001b[1;32m----> 5\u001b[1;33m           shuffle=True, epochs=10, callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"weights.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit([train_text], batch_size=64, y=to_categorical(train_label), verbose=1, validation_split=0.10, \n",
    "          shuffle=True, epochs=10, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 13844s 22s/step - loss: 3.1354 - acc: 0.2834 - val_loss: 2.2710 - val_acc: 0.3079\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 9910s 16s/step - loss: 2.2011 - acc: 0.1156 - val_loss: 2.0819 - val_acc: 0.0721\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 9810s 16s/step - loss: 2.0097 - acc: 0.1434 - val_loss: 1.7269 - val_acc: 0.2140\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 9660s 15s/step - loss: 1.5302 - acc: 0.2795 - val_loss: 1.4195 - val_acc: 0.3734\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 9593s 15s/step - loss: 1.2913 - acc: 0.3592 - val_loss: 1.3584 - val_acc: 0.3843\n"
     ]
    }
   ],
   "source": [
    "logfile_path = './log'\n",
    "tb_cb = TensorBoard(log_dir=logfile_path, histogram_freq=0)\n",
    "\n",
    "history = model.fit_generator(train_batches, train_steps,\n",
    "                              epochs=5, \n",
    "                              validation_data=valid_batches,\n",
    "                              validation_steps=valid_steps,\n",
    "                              callbacks=[tb_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying AppInsights with name nnworkspinsightsf45ea59a.\n",
      "Deployed AppInsights with name nnworkspinsightsf45ea59a.\n",
      "Deploying KeyVault with name nnworkspkeyvault2eb76ef6.\n",
      "Deploying StorageAccount with name nnworkspstorage0b992773b.\n",
      "Deployed KeyVault with name nnworkspkeyvault2eb76ef6.\n",
      "Deploying Workspace with name nnworkspace.\n",
      "Deployed StorageAccount with name nnworkspstorage0b992773b.\n",
      "Deployed Workspace with name nnworkspace.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment, Workspace\n",
    "\n",
    "ws = Workspace.create(name='nnworkspace',\n",
    "                      subscription_id='26268c67-6923-4060-b75e-e672e201df32',\n",
    "                      resource_group='nnproject',\n",
    "                      create_resource_group=True,\n",
    "                      location='eastus' \n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new experiment in your workspace.\n",
    "exp = Experiment(workspace=ws, name='exp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a new compute target...\n",
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-05-21T17:30:13.530000+00:00', 'errors': None, 'creationTime': '2019-05-21T17:29:37.016645+00:00', 'modifiedTime': '2019-05-21T17:30:32.392339+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpucluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureBlob nnworkspstorage0b992773b azureml-blobstore-4f341f39-327b-4d16-ae7c-2140930849f1\n",
      "Uploading C:\\Users\\Adil\\Desktop\\PROJECT\\Untitled Folder\\newsdata\\News_Category_Dataset_v2.json\n",
      "Uploaded C:\\Users\\Adil\\Desktop\\PROJECT\\Untitled Folder\\newsdata\\News_Category_Dataset_v2.json, 1 files out of an estimated total of 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scripts_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-422c4147c2c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mscript_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'scripts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscripts_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscript_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'scripts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scripts_folder' is not defined"
     ]
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'newsdata')\n",
    "os.makedirs(data_folder, exist_ok = True)\n",
    "\n",
    "ds.upload(src_dir=data_folder, target_path='newsdata', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading C:\\Users\\Adil\\Desktop\\PROJECT\\Untitled Folder\\scripts\\train_elmo.py\n",
      "Uploaded C:\\Users\\Adil\\Desktop\\PROJECT\\Untitled Folder\\scripts\\train_elmo.py, 1 files out of an estimated total of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_02b459f809d34dbc8356ddb0a95cd9eb"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_folder = os.path.join(os.getcwd(), 'scripts')\n",
    "os.makedirs(script_folder, exist_ok = True)\n",
    "ds.upload(src_dir=script_folder, target_path='scripts', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "framework_version is not specified, defaulting to version 1.13.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "\n",
    "# os.makedirs(data_folder, exist_ok = True)\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                script_params=None,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train_elmo.py',\n",
    "                conda_packages=['pandas', 'tensorflow','scikit-learn'],\n",
    "                pip_packages=['keras','tensorflow-hub'],\n",
    "                use_gpu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.delete(delete_dependent_resources=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>exp2</td><td>exp2_1558460117_560f57a8</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/26268c67-6923-4060-b75e-e672e201df32/resourceGroups/nnproject/providers/Microsoft.MachineLearningServices/workspaces/nnworkspace/experiments/exp2/runs/exp2_1558460117_560f57a8\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: exp2,\n",
       "Id: exp2_1558460117_560f57a8,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089ba493d1a044c9ac7e112c26b0aae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
