{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adil\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras_preprocessing import sequence\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import Input\n",
    "from tensorflow.python.keras.layers import Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras_preprocessing import sequence\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import Input\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLACK VOICES', 'COMEDY', 'HEALTHY LIVING', 'SPORTS', 'WORLD NEWS'}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('train_.txt') as json_file:  \n",
    "    train_data = json.load(json_file)\n",
    "\n",
    "from random import shuffle\n",
    "shuffle(train_data)\n",
    "\n",
    "\n",
    "\n",
    "a=[]\n",
    "for i in train_data:\n",
    "    a.append(i['category'])\n",
    "classes=set(a)\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=train_data[0:1000]\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {}\n",
    "for k,v in enumerate(classes):\n",
    "    d[v] = k\n",
    "    \n",
    "k=[]\n",
    "C=[]\n",
    "for item in train_data:\n",
    "    c=d[item['category']]\n",
    "    C.append(c)\n",
    "    EXP=item['headline']+item['short_description']\n",
    "    k.append(EXP)\n",
    "\n",
    "train_df = pd.DataFrame(k)\n",
    "train_df['category']=C\n",
    "train_df.columns=[\"sentence\",\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Babies Given Antibiotics Are More Likely To De...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Moonlight' Director Still Hasn't Gotten Over ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legionnaires' Disease Is Back In NYC With New ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jimmy Kimmel Launches A New Personal Crusade.....</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 Thanksgiving Recipes To Poison Your Uncle Ri...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  category\n",
       "0  Babies Given Antibiotics Are More Likely To De...         1\n",
       "1  'Moonlight' Director Still Hasn't Gotten Over ...         4\n",
       "2  Legionnaires' Disease Is Back In NYC With New ...         1\n",
       "3  Jimmy Kimmel Launches A New Personal Crusade.....         2\n",
       "4  5 Thanksgiving Recipes To Poison Your Uncle Ri...         2"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "time_steps = 100\n",
    "\n",
    "# building vocabulary from dataset\n",
    "def build_vocabulary(sentence_list):\n",
    "    unique_words = \" \".join(sentence_list).strip().split()\n",
    "    word_count = Counter(unique_words).most_common()\n",
    "    vocabulary = {}\n",
    "    for word, _ in word_count:\n",
    "        vocabulary[word] = len(vocabulary)        \n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = build_vocabulary(train_df[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets (Only take up to time_steps words for memory)\n",
    "train_text = train_df['sentence'].tolist()\n",
    "train_text = [' '.join(t.split()[0:time_steps]) for t in train_text]\n",
    "train_text = np.array(train_text)\n",
    "train_label = np.array(train_df['category'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LENGTH = 200\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "post_seq = tokenizer.texts_to_sequences(train_text)\n",
    "post_seq_padded = pad_sequences(post_seq, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(post_seq_padded, train_label, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(5)\n",
    " \n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        attention_weights = tf.reduce_max(attention_weights, axis=2)\n",
    "        attention_weights = tf.expand_dims(attention_weights, -1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    " \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    " \n",
    "embedded_sequences = keras.layers.Embedding(len(vocabulary), 16, input_length=max_len)(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM\n",
    "                                     (rnn_cell_size,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_activation='relu',\n",
    "                                      recurrent_initializer='glorot_uniform'), name=\"bi_lstm_0\")(embedded_sequences)\n",
    " \n",
    "lstm, forward_h, forward_c, backward_h, backward_c = tf.keras.layers.Bidirectional \\\n",
    "    (tf.keras.layers.LSTM\n",
    "     (rnn_cell_size,\n",
    "      dropout=0.2,\n",
    "      return_sequences=True,\n",
    "      return_state=True,\n",
    "      recurrent_activation='relu',\n",
    "      recurrent_initializer='glorot_uniform'))(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 200, 16)      153984      input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       [(None, 200, 256), ( 148480      embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional [(None, 200, 256), ( 394240      bi_lstm_0[0][0]                  \n",
      "                                                                 bi_lstm_0[0][1]                  \n",
      "                                                                 bi_lstm_0[0][2]                  \n",
      "                                                                 bi_lstm_0[0][3]                  \n",
      "                                                                 bi_lstm_0[0][4]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_132 (Concatenate)   (None, 256)          0           bidirectional_12[0][1]           \n",
      "                                                                 bidirectional_12[0][3]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_57 (Attention)        [(None, 256), (None, 16613       bidirectional_12[0][0]           \n",
      "                                                                 concatenate_132[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_184 (Dense)               (None, 5)            1285        attention_57[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 714,602\n",
      "Trainable params: 714,602\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "attention = Attention(32)\n",
    " \n",
    "context_vector, attention_weights = attention(lstm,state_h)\n",
    " \n",
    "output = keras.layers.Dense(5, activation='softmax')(context_vector)\n",
    " \n",
    "model = keras.Model(inputs=sequence_input, outputs=output)\n",
    " \n",
    "# summarize layers\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 712 samples, validate on 238 samples\n",
      "Epoch 1/5\n",
      "712/712 [==============================] - 77s 108ms/sample - loss: 1.6078 - acc: 0.2683 - val_loss: 1.6048 - val_acc: 0.2521\n",
      "Epoch 2/5\n",
      "712/712 [==============================] - 86s 121ms/sample - loss: 1.5961 - acc: 0.3146 - val_loss: 1.5857 - val_acc: 0.2521\n",
      "Epoch 3/5\n",
      "712/712 [==============================] - 95s 133ms/sample - loss: 1.5530 - acc: 0.3146 - val_loss: 1.5866 - val_acc: 0.2521\n",
      "Epoch 4/5\n",
      "712/712 [==============================] - 91s 127ms/sample - loss: 1.5408 - acc: 0.3146 - val_loss: 1.5520 - val_acc: 0.2521\n",
      "Epoch 5/5\n",
      "712/712 [==============================] - 92s 130ms/sample - loss: 1.5376 - acc: 0.3146 - val_loss: 1.5565 - val_acc: 0.2521\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train,\n",
    "                    to_categorical(y_train),\n",
    "                    epochs=5,\n",
    "                    batch_size=64,\n",
    "                    validation_split=.25, verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
